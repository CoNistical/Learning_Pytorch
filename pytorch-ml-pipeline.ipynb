{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# From the delivery example if we wanted to increase our deliveries 100x\n# All of this will get loaded into memory\nall_distances = []\nall_times = []\n\nfor i in range(100000):\n    distance, time = load_delivery_record(i)\n    all_distance.append(distance)\n    all_times.append(time)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# It is better to load data into batches\n# We can start by transforming the data which standardizes the data and brings it closer to zero\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((mean,), (std,))\n])\n\n# Next we wrap that into a Dataset object\ntrain_dataset = SomeDataset('./data', train=True, download=True, transform=transform)\n\n# We can then index the data\nfirst_item = dataset[0]\n\n# Once we load the data set we use a DataLoader to break it into batches\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Then we can train!!!!\nfor batch_idx, (data, labels) in enumerate(train_loader):\n    # The data arrives in batches, and is already transformed\n    # The model processes the batch\n    output = model(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We used the following when we were handling Sequential data\nmodel = nn.Sequential(\n    nn.Linear(1, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n)\n\n# But there is a better way to handle data - this gives us better control\nclass ExampleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(1, 20)\n        self.relu = nn.ReLU()\n        self.layer_2 = nn.Linear(20, 1)\n\n    def forward(self, x):\n        x = self.layer_1(x)\n        x = self.relu(x)\n        x = self.layer_2(x)\n        return x\n\n# We can then run\nmodel = ExampleModel()\noutput = model(data) # PyTorch handles everything for us\n\n# Then we train\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = model(X)\n    loss = loss_functions(outputs, y)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How can we debug and test model on new data?\nmodel.eval() # Set evaluation mode (Is not evaluate my mode)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for image, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1) # Get the class with the highest score\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    print(f'Accuracy: {accuracy}%')\n\n# Switching the model back to training mode\nmodel.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}