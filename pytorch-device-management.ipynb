{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Checking if GPU is available\ntorch.cuda.is_available() # If True then the GPU is available","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Moving the model onto the device\nmodel = MyModel().to(device)\n\n# Then in training we move each batch\nfor inputs, targets in dataloader:\n    inputs = inputs.to(device)\n    targets = targets.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking\n# For tensors\nprint(inputs.device)\n\n# For models\nprint(next(model.parameters()).device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Complete training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MyModel.to(device)\noptimizer = optim.Adam(model.parameters())\nloss_function = nn.CrossEntropyLoss()\n\nfor inputs, targets in dataloader:\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n\n    optimizer.zero_grade()\n    outputs = model(inputs)\n    loss = loss_function(outputs, targets)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}